%In the following, we 

%\color{blue}




%\revised{:q

In this section, we report on the materials and methods used to evaluate \TF. %evaluation of \CT, having the {\em goal} of evaluating the performance of the proposed approach. 
Section~\ref{sec:Dataset} presents the datasets exploited in the evaluation. Section~\ref{sec:methodology-metric} and Section~\ref{sec:metrics} describe the methodology and metrics, respectively. Finally, the research questions are highlighted in Section~\ref{sec:ResearchQuestions}.

To facilitate future research, we made available the \TF tool together with the related data in a GitHub repository\footnote{https://github.com/ESEM2020-TopFilter/TopFilter}.

%ten equal parts, so-called folds
%We also exploited 
%was also exploited 

\begin{figure*}[h!]
	\centering
	\includegraphics[width=0.9\linewidth,keepaspectratio]{figs/evaluationCF.pdf}
	%\vspace{-.3cm}
	\caption{Evaluation Process.}
	\label{fig:EvaluationProcess}
	\vspace{-.3cm}
\end{figure*}

\subsection{Data Extraction} \label{sec:Dataset}



%\subsection{Data Cleaner}  \label{sec:filter}

As a preprocessing, we filter the initial set of topics using their frequencies counted on the entire \GH dataset. We remove irrelevant topics to reduce the noise in the prediction phase. Through the \emph{cut-off} value, we progressively increase the frequency threshold to evaluate possible impacts on overall performances. As stated in \cite{repo-topix}, this preprocessing can improve the final results, thus we decide to apply it as a first step. 

To this end, we develop tailored Python scripts that apply this filter to the initial dataset. As a \GH user can manually specify the topic list for his repository, a lot of them can contain infrequent or improper terms \ie the name of the author, duplicated values, terms that rarely appear to name a few. On one hand, imposing such preprocessing reduces the repositories to analyze as well as topics to recommend. On the other hand, we improve the overall quality of recommendation by pruning "bad" terms. 
This pruning phase is computed offline and doesn't affect the time required for the recommendation process. 

%Before encoding topics in the repo-topic ratings matrix, raw topics are pre-processed removing possible syntactical duplicates terms (\eg \textit{document} and \textit{documents}). More specifically, stemming, lemmatization, and stop words removal Natural Language Processing (NLP) techniques have been applied on the mined topics.




%\color{blue}

To evaluate the approach, we reuse the same dataset employed for the \MNB available here \cite{MNBreplication}. The \GH query language \cite{understanding} allows the fetching of relevant repository metadata including name, owner, and list of topics to mention a few. Thus, we \emph{randomly} collected a dataset consisting of 6,258 repositories that use 15,757 topics by means of the GitHub API \cite{pygithub/pygithub_2019}. We employ the \GH star voting mechanism as a popularity measure to avoid including unpopular, unmaintained and toy projects \cite{borges_whats_2018}. As claimed in several works\cite{borges_popularity_2017, borges_predicting_2016}, a high number of stars means the attention of the community for that project. So, we impose the following filter during the query execution:
\begin{equation}
\small
Qf = "is:featured \; topic:t \; stars:100..80000 \; topics:>=2"
\end{equation}%
to consider only \GH repositories having a number of stars between 100 and 80,000, and tagged with at least two topics. The boolean qualifier \emph{is:featured} is used in the \MNB work to group repositories given a certain featured topic (please refers to \url{https://github.com/topics} for the complete list of featured topics). As \CT is able to retrieve both featured and not-featured topics, this filter doesn't affect the quality of the collected data.
To investigate the \CT prediction performances, we populated five different datasets starting from the original one by varying the topic frequency cut-off value \emph{t} \ie the maximum frequency of the topic distribution (it will be better described in Section~\ref{sec:methodology-metric}). In this way, we remove the infrequent elements from the dataset to analyze the impacts on the recommendation phase as well as on the composition of the dataset. Table \ref{tab:Datasets} summarizes the datasets' features with \emph{t} = 1, 5, 10, 15, 20. In particular, for each dataset the \emph{Avg. topics} values are the average number of topics that the repositories include, and the  \emph{Avg. freq. topics} values are the average frequency of the topics in the dataset, \ie the average occurrences of the topics among  the considered repositories in the dataset.


%\begin{table}[h]
%\centering
%\resizebox{8.5cm}{!} {
%\begin{tabular}{|l|l|l|c|c|}
%\hline
%\textbf{Dataset} & \textbf{No. of repos} &\textbf{ No. of topics} & \textbf{Avg topics for repo} & \textbf{Avg freq. for topic} \\ \hline
%D$_1$  &       6,253      &    15,743       &  9.9  &  3.9       \\ \hline
% D$_5$  &        3,884      &    1,989      &     8.4  &   16.5   \\ \hline
%D$_{10}$  &    2,897         &      964	     &   8.0    &  24.1 \\ \hline
%D$_{15}$  &    2,273        &   634       &   7.8  &  28.1       \\ \hline
%D$_{20}$  &    1,806       &   456        &   7.7 &  30.5       \\ \hline
%
%\end{tabular}
%}
%\caption{Datasets' description.}
%\label{tab:datasets}
%\end{table} 


\begin{table}[h!]
%	\small
	\caption{Datasets.}
	\begin{tabular}{|l|p{0.68cm}|p{0.68cm}|p{0.68cm}|p{0.68cm}|p{0.68cm}|} \hline
		 & \textbf{ D$_{1}$} & \textbf{D$_{5}$} & \textbf{ D$_{10}$} & \textbf{D$_{15}$} & \textbf{D$_{20}$} \\ \hline
		Number of repos & 6,253 & 3,884 & 2,897  & 2,273 & 1,806  \\ \hline
		Number of topics & 15,743 & 1,989 & 964 & 634 & 456 \\ \hline
		Avg. topics & 9.9 & 8.4 & 8.0  & 7.8 & 7.7 \\ \hline
		Avg. freq. topics & 3.9  & 16.5  & 24.1  & 28.1  & 30.5  \\ \hline
	\end{tabular}	
	\label{tab:Datasets}	
\end{table}



%<<<<<<< HEAD
As we can see in the next section, removing the infrequent topics improves the overall quality of the considered datasets. 
Similarly to the other collaborative filtering approaches, the overall prediction performance strongly depends on the dataset. As we will demonstrate in the next section, the collaborative filtering provides better prediction performance when there are enough data (\ie topics) in the training set to resemble the repository behaviour. After infrequent topics is removed, the repository that consist of less then 5 topics are filter out from the dataset because they contain very few information to enable the collaborative filtering prediction. In particular, we remove around 2,300 repositories by increasing the cut-off value from 1 to 5. It means that the excluded repositories in Dataset D$_5$ are tagged with topics that rarely appear in the considered repositories. This finding is strengthened by the number of topics, which dramatically decreases to 1,989. The other datasets confirm this trend even though the delta of removed repositories goes down at each filtering step. Thus, we stop at t=20 and consider Dataset D$_{20}$ as the best one according to our metrics. Additionally, we observe that repositories are tagged by 9.9 and 7.7 topics on average for \emph{t} = 1 and \emph{t} = 20 respectively. This demonstrates that a huge number of topics doesn't help the discoverability of a project.

\begin{figure*}[h!]
\centering
	\begin{tabular}{c c }	
	\subfigure[Dataset D$_5$]{\label{fig:dt5_stars}
	\includegraphics[width=0.45\linewidth]{figs/dataset_5_stars.pdf}} &
	\subfigure[Dataset D$_{10}$]{\label{fig:dt10_stars}
	\includegraphics[width=0.45\linewidth]{figs/dataset_10_stars.pdf}} \\
\subfigure[Dataset D$_{15}$]{\label{fig:dt15_stars}
	\includegraphics[width=0.45\linewidth]{figs/dataset_15_stars.pdf}} & 	
\subfigure[Dataset D$_{20}$]{\label{fig:dt20_stars}
	\includegraphics[width=0.45\linewidth]{figs/dataset_20_stars.pdf}}\\	
	\end{tabular} 
	\caption{Quality analysis of the examined datasets.}
	\label{fig:comparison}
\end{figure*}

 
%=======
%As we can see, removing the infrequent topics improves the overall quality of the considered datasets. In particular, we remove around 2,300 repositories by increasing the cut-off value from 1 to 5. It means that the excluded repositories in Dataset D$_5$ are tagged with topics that rarely appear in the considered repositories. This finding is strengthened by the number of topics, which dramatically decreases to 1,989. The other datasets confirm this trend even though the delta of removed repositories goes down at each filtering step. Thus, we stop at t=20 and consider Dataset D$_{20}$ as the best one according to our metrics. Additionally, we observe that repositories are tagged by 7 topics on average. This demonstrates that a huge number of topics doesn't help the discoverability of a project. 
%>>>>>>> 2241e0e812c956f441252b061dd0d75d6f6927ea
Furthermore, we evaluate the quality of the OSS project belonging to the examined dataset. As mentioned before, the \GH community assesses this aspect by mainly using forks and stars. Thus, we collect this data for each dataset using the same Github API library employed for the crawling. Figure \ref{fig:comparison} shows the comparison among all the examined datasets. As can see, filtering repositories by the \emph{t} value helps to smooth the distribution. On one hand, the Dataset D$_5$ contains more repositories with a high forks number rather than the ultimate dataset \ie it reaches around 20,000 forks against 15,000 with t=5 and t=20 respectively. On the other hand, the slope depicted in Dataset D$_{20}$ is higher than the original dataset. The positive trend is confirmed by observing the distribution of the other datasets \ie Dataset D$_{10}$ and D$_{15}$. In particular, we are able to remove repositories with less number of stars \ie from 5,000 to 10,000. 
From this study, we observe a correlation between the number of stars and the topics' frequency. In other words, most frequent topics appear in the high-ranked repositories and this finding affects the quality of the recommendations. 







%Among these projects, only $7$ of them have been forked from other projects. Such original projects have been excluded from the dataset as their forked ones share highly similar libraries, and this may introduce bias in the recommendation outcomes. We represent the distribution of projects with respect to the number of forks, commits and pull requests in Fig.~\ref{fig:ForkCommitPull}. Most projects have a low number of pull requests, \ie lower than 100, however many of them have a large number of forks and commits. Forking is a means to contribute to the original repositories~\cite{Jiang:2017:WDF:3042021.3042043}. Furthermore, there is a strong correlation between forks and stars~\cite{7816479}, as it is further witnessed in Fig.~\ref{fig:ForkStarIssue}. A project with a high number of forks means that it gets attention from the OSS community. In this sense, having many forks can be considered as a sign of a well-maintained and well-received project. Meanwhile, as commits have an impact on the source code~\cite{8009930}, the number of commits is also a good indicator of how a project has been developed.
%}


%e mined dependency specification by means of \code{code.xml} or \code{.grad\-le} files.\footnote{The files \code{pom.xml} and with the extension \code{.gradle} are related to management of dependencies by means of Maven (\url{https://maven.apache.org/}) and Gradle (\url{https://gradle.org/}), respectively.} Fig.~\ref{fig:NumOfLibsD1} depicts the distribution of libraries across the projects. Most of the libraries in \code{D1} ($12,962$) are used by a small number of projects, and only $10$ libraries are extremely popular by being included in more than $200$ projects. By carefully investigating the dataset, we also see that most projects contain a small number of dependencies, \ie $48\%$ of the projects include less than $20$ libraries and just $15\%$ of them include more than $100$ libraries. \}


%\color{blue}
\subsection{Evaluation process}\label{sec:methodology-metric}
The \emph{ten-fold cross-validation} methodology~\cite{kohavi1995study} has been used to assess the performance of \CT, \MNB and combined approach where every time $9$ folds 
%(each one contains $625$ projects) 
are used for training and the remaining one for testing.
For each testing project \emph{p}, we randomly delete half topics and save it as ground truth (\emph{GT(p)}) . The ground truth data  will be used to validate the recommendation outcomes. The remaining half topics are used as query topics to the \CT.
Figure~\ref{fig:EvaluationProcess} depicts the evaluation process consists of three consecutive phases,~\ie~\emph{Data Preparation},~\emph{Recommendation}, and~\emph{Outcome Evaluation}.

\noindent
$\blacksquare$~\textbf{Data Preparation phase} collects repositories that match the requirements defined in previous section from GitHub during \code{Data collection} step. This dataset is used to evaluate \CT, \MNB, and the combination of two. 
The dataset is then split into training and testing sets (\ie \code{ Split ten-fold} activity).
Due to the different nature of the the recommender systems (\ie \MNB requires \code{README} files as input and training data, whereas \CT uses a set of assigned topics as input and for training the recommendation system), the testing and training data are specifically cooked for both approaches).
The \code{Split topic} activity resembles a real development process where a developer has already included some topics in his repository (\ie \code{Query topics}) and waits for recommendations \ie additional topics to be incorporated. \CT recommender system is expected to provide her with the other half, \ie \code{ GT topics}.

\noindent
$\blacksquare$~\textbf{Recommendation phase} follows three different flows, according to the required input and produced output of the three mentioned approaches. In particular, the common operations are in white while the three different evaluation flows are represented in a grayscale fashion (\ie light grey, grey and dark grey boxes are related to \code{\CT}, \code{\MNB}, and \code{Entangled} approaches evaluation respectively).
To enable \CT, we extract a portion of topics from a given testing project \ie the ground-truth part (it is defined as $GT(p)$ in the following). The left part is used as a query to produce recommendations (see the dotted line flow). As the \MNB uses the README file of a repository to predict a set of topics, this doesn't require any topic as input. Thus, the approach encodes the document relevant information in vectors using the TF-IDF weighting scheme. Then, to feed the network that delivers a set of topics (see the bold line). Finally, the entangled approach uses \CT as the recommendation engine which is fed by the \MNB suggested topics (see dashed line flow). In this respect, both \code{Testing data} and \code{Training set} boxes are simplified to provides the needed data (\ie README file and assigned topics) to the different recommender systems.

\noindent
$\blacksquare$~\textbf{Outcome Evaluation phase} evaluate the recommendation results with those stored as ground-truth data to compute the quality metrics (\ie \code{Success rate}, \code{Precision}, \code{Recall}, and \code{Catalog coverage}) during the \code{Comparison} activity.  

It is worth noting that we can't directly compare directly \CT and \MNB approaches because they rely on different input data (\ie \CT requires an initial set of assigned topics for suggesting new ones, whereas\MNB uses the information mined from the README files to recommend the expected topics). 
%, which are called \emph{\textbf{te}}, and serve as input for \code{Similarity Computation} and \code{Recommendation} components.


\subsection{Metrics' definition}\label{sec:metrics}

In the context of recommendation systems, several metrics have been proposed to evaluate a ranked list of recommended items \cite{DBLP:conf/rweb/NoiaO15}. In the scope of this paper, \emph{success rate} \emph{accuracy}, and \emph{catalog coverage} have been used to study the systems' performance as already proposed in \etal~\cite{Robillard:2014:RSS:2631387}

% and other studies~\cite{6671293},\cite{Nguyen:2015:ESP:2740908.2742141}.
%Before defining the metrics involved in the experiment, we present the following notations to make easier the understanding of the considered metrics:
First of all, we present the following notations to make easier the understanding of the metrics involved in our experiment:
\begin{itemize}[noitemsep,topsep=0pt]
	\item \emph{t} is the frequency cut-off value of input topics (\ie all topics that occur less than \emph{t} times are removed from the dataset)
	\item |\emph{t$_{in}$}| is the size of topics that \CT takes as input;
	\item \emph{N} is the cut-off value for the recommended ranked list of topic;% of recommended libraries;
	\item \emph{k} is the number of top-similar neighbour projects \TF considers to predict suggested topics;
	\item \emph{GT(r)} is defined as the half part of the extracted topics for a testing project \emph{r}, and \emph{GT(r)} have been used as ground-truth data in the evaluation process;
%	\item For a testing project \emph{r}, a half of its topics are extracted and used as the ground-truth data named as \emph{GT(r)};
	\item $REC_{N}(r)$ is the outcome of \TF to a given repository \emph{r}, \ie the top-\emph{N}  suggested topics in a descending order by considering the recommendation scores;
	%\item $REC(r)$ is the \emph{top-N} topics recommended to a repository \emph{r}. It is a ranked list in descending order of real scores;
	\item a recommended topic \emph{rt} to a repository \emph{r} is marked as a \emph{match} if $rt \in REC(r)$;
	\item  $match_{N}(r)$ is the set of items in $REC_{N}(r)$ that match with those in \emph{GT(r)} for a given a repository \emph{r}.
\end{itemize}



\noindent By using the such notations, the accuracy, success rate and coverage  metrics are defined as follows.  

\JDR{Rephrase this section}

\vspace{.1cm}
\noindent\textbf{Success rate@N.} Success rate is defined as the ratio of queries that have at least a matched topic among the total number of attempts. In particular, given a set of testing projects \emph{P}, this metric measures the percentage of \emph{top-N} query results that match at least one topic in the \emph{GT(p)} among the number of query project $p \in P$~\cite{6671293}: %It is formally defined as given below:
\vspace{-.1cm}

\begin{equation} \label{eqn:RecallRate}
success\ rate@N=\frac{ count_{p \in P}( \left | match_{N}(p) \right | > 0 ) }{\left | P \right |} %\times 100\%
%success\ rate@N=\frac{ count_{p \in P}( \left | GT(p) \bigcap (\cup_{r=1}^{N} REC_{r}(p)) \right | > 0 ) }{\left | P \right |}
\end{equation}

\vspace{.1cm}
\vspace{.1cm}




%\noindent \JDR{Check if it will be used} \textbf{Success rate$_M$@N.} Given a set of testing projects \emph{P}, this metric measures the rate at which a recommender system returns at least $M$ topics match among \emph{top-N} items for every project $p \in P$ \cite{6671293}: %It is formally defined as given below:
%\vspace{-.1cm}
%
%\begin{equation} \label{eqn:RecallRate}
%success\ rate_M@N=\frac{ count_{p \in P}( \left | match_{N}(p) \right | >= M ) }{\left | P \right |} %\times 100\%
%%success\ rate@N=\frac{ count_{p \in P}( \left | GT(p) \bigcap (\cup_{r=1}^{N} REC_{r}(p)) \right | > 0 ) }{\left | P \right |}
%\end{equation}
%
%\vspace{.1cm}
%\noindent where the function \emph{count()} counts the number of times that the boolean expression specified in its parameter is \emph{true}.

\noindent \textbf{Accuracy.} Because of \emph{success rate@N} only measures successful queries and it does not reflect how accurate the outcome of a recommender system is
(\eg considering two recommendation results where the former counts $1$ topic match out of $5$ and the latter that returns all $5$ topic matches,  both results equally affect the success rate value because both results include at least one matched topic.), accuracy has been considered as further quality indicator for \TF. Given a list of \emph{top-N} libraries, \emph{precision@N} and \emph{recall@N} are utilized to measure the \emph{accuracy} of the recommendation results. \emph{precision@N} is the ratio of the \emph{top-N} recommended topics belonging to the ground-truth dataset, whereas \emph{recall@N} is the ratio of the ground-truth topics appearing in the \emph{N} recommended items \cite{Nguyen:2019:FRS:3339505.3339636},\cite{DiNoia:2012:LOD:2362499.2362501},\cite{Davis:2006:RPR:1143844.1143874}:

%However, \emph{success rate@N} does not reflect how accurate the outcome of a recommender system is. For instance, given only one testing project, there is no difference between a system that returns $1$ topic match out of $5$ and another system that returns all $5$ topic matches, since \emph{success rate@5} is $100\%$ for both cases (see Eq.~\eqref{eqn:RecallRate}). Thus, given a list of \emph{top-N} libraries, \emph{precision@N} and \emph{recall@N} are utilized to measure the \emph{accuracy} of the recommendation results. \emph{precision@N} is the ratio of the \emph{top-N} recommended topics belonging to the ground-truth dataset, whereas \emph{recall@N} is the ratio of the ground-truth topics appearing in the \emph{N} recommended items \cite{Nguyen:2019:FRS:3339505.3339636},\cite{DiNoia:2012:LOD:2362499.2362501},\cite{Davis:2006:RPR:1143844.1143874}: %,Nguyen:2015:CRV:2942298.2942305 %}. A concrete definition for the metrics is given below \cite{

%\vspace{-.2cm}
% \cite{Saracevic:1995:EEI:215206.215351}

\begin{equation} \label{eqn:Precision}
precision@N = \frac{ \left | match_{N}(p) \right | }{N}
%precision@N(p) = \frac{\sum_{r=1}^{N}\left | GT(p) \bigcap REC_{r}(p) \right |}{N}
\end{equation}
%\vspace{-.1cm}
\begin{equation} \label{eqn:Recall}
recall@N = \frac{ \left | match_{N}(p) \right | }{\left | GT(p) \right |}
%recall@N(p) = \frac{\sum_{r=1}^{N}\left | GT(p) \bigcap REC_{r}(p) \right |}{\left | GT(p) \right |}
\end{equation}
%\vspace{-.1cm}

\vspace{.1cm}
\vspace{.1cm}



\noindent \textbf{Catalog coverage.} This metric is particularly suitable to measure the performance predictions of recommendation systems that suggest a list of items \cite{ge_beyond_2010}. Given the set of projects $U_p$, we compare the number of recommended topics with the global number of the available ones \ie $ REC_{N}(p)$ and T respectively. Reversely from the previous two metrics, the Catalog Coverage measures the suitability of the delivered topics considering all the possible set of values. From the evaluation point of view, it is interesting to assess the impact of N value on the coverage stability, meaning what values of N impacts on the overall prediction performances. 



\begin{equation}\label{eqn:Coverage}
%coverage = \frac{\left | \cup_{p\in P} \left [  \cup_{r=1}^{N} REC_{r}(p) \right ] \right | }{\left | L \right |} 
coverage@N = \frac{\left | \cup_{p\in P} REC_{N}(p) \right | }{\left | T \right |} 
\end{equation}

\vspace{.1cm}
\vspace{.1cm}





%To assess the performance of \CR proposed approach, we applied ten-fold cross-validation, considering every time $9$ folds (each one contains $625$ projects) for training and the remaining one for testing. 
%	For every testing project \emph{p}, a half of its topics are \emph{randomly} 
%	taken out and saved as ground truth data, let us call them \emph{GT(p)}, which will be used to validate the recommendation outcomes. The other half are used 
%	as testing libraries or query, which are called \emph{\textbf{te}}, and serve 
%	as input for \code{Similarity Computation} and \code{Recommendation}.
%The splitting mimics a real development process where a 
%developer has already included some topics in the current project, \ie 
%\emph{\textbf{te}} and waits for recommendations, that means additional topics to be incorporated. A recommender system is expected to provide her 
%with the other half, \ie \emph{GT(p)}. %To ensure a reliable comparison between \LR and \CR, we performed cross-validation for both using exactly the same folds.
%
%
%
%There are several metrics available to evaluate a ranked list of recommended items \cite{DBLP:conf/rweb/NoiaO15}. In the scope of this paper, \emph{success rate}, \emph{accuracy}, \emph{sales diversity}, and \emph{novelty} have been used to study the systems' performance as already proposed by Robillard \etal~\cite{Robillard:2014:RSS:2631387} and other studies~\cite{6671293},\cite{Nguyen:2015:ESP:2740908.2742141}. For a clear presentation of the metrics considered during the outcome evaluation, let us introduce the following notations:
%
%\begin{itemize}[noitemsep,topsep=0pt]
%	\item \emph{N} is the cut-off value for the ranked list;% of recommended libraries;
%	\item \emph{k} is the number of neighbor projects exploited for the recommendation process;
%	\item For a testing project \emph{p}, a half of its libraries are extracted and used as the ground-truth data named as \emph{GT(p)};
%	\item $REC(p)$ is the \emph{top-N} libraries recommended to \emph{p}. It is a ranked list in descending order of real scores;%, with $REC_r(p)$ being the recommended library in the position $r$.		
%	\item If a recommended library $l \in REC(p)$ for a testing project $p$ is found in the ground truth of $p$ (\ie \emph{GT(p)}), hereafter we call this as a library \textit{match} or \textit{hit}.	
%\end{itemize}	
%
%
%
%If $REC_{N}(p)$ is the set of top-$N$ items and $match_{N}(p)=  GT(p) \bigcap REC_{N}(p) $ is the set of items in the \emph{top-N} list that match with those in the ground-truth data, then the metrics are defined as follows.
%
%\paragraph{\textbf{Success rate@N}} Given a set of testing projects \emph{P}, this metric measures the rate at which a recommender system returns at least a topic match among \emph{top-N} items for every project $p \in P$ \cite{6671293}: %It is formally defined as given below:
%\vspace{-.3cm}
%
%\begin{equation} \label{eqn:RecallRate}
%success\ rate@N=\frac{ count_{p \in P}( \left |  match_{N}(p) \right | > 0 ) }{\left | P \right |} %\times 100\%
%%success\ rate@N=\frac{ count_{p \in P}( \left |  GT(p) \bigcap (\cup_{r=1}^{N} REC_{r}(p)) \right | > 0 ) }{\left | P \right |} 
%\end{equation}
%
%\noindent where the function \emph{count()} counts the number of times that the boolean expression specified in its parameter is \emph{true}.
%
%
%\paragraph{\textbf{Accuracy}} Accuracy is considered as one of the most preferred \emph{quality indicators} for Information Retrieval applications \cite{Saracevic:1995:EEI:215206.215351}. However, \emph{success rate@N} does not reflect how accurate the outcome of a recommender system is. For instance, given only one testing project, there is no difference between a system that returns $1$ topic match out of $5$ and another system that returns all $5$ topic matches, since \emph{success rate@5} is $100\%$ for both cases (see Eq.~\eqref{eqn:RecallRate}). Thus, given a list of \emph{top-N} libraries, \emph{precision@N} and \emph{recall@N} are utilized to measure the \emph{accuracy} of the recommendation results. \emph{precision@N} is the ratio of the \emph{top-N} recommended topics belonging to the ground-truth dataset, whereas \emph{recall@N} is the ratio of the ground-truth topics appearing in the \emph{N} recommended items \cite{Nguyen:2019:FRS:3339505.3339636},\cite{DiNoia:2012:LOD:2362499.2362501},\cite{Davis:2006:RPR:1143844.1143874}: %,Nguyen:2015:CRV:2942298.2942305 %}. A concrete definition for the metrics is given below \cite{
%
%%\vspace{-.2cm}
%% \cite{Saracevic:1995:EEI:215206.215351}
%
%\begin{equation} \label{eqn:Precision}
%precision@N = \frac{ \left |  match_{N}(p) \right | }{N}
%%precision@N(p) = \frac{\sum_{r=1}^{N}\left |  GT(p) \bigcap REC_{r}(p) \right |}{N}
%\end{equation}
%%\vspace{-.1cm}
%\begin{equation} \label{eqn:Recall}
%recall@N = \frac{ \left |  match_{N}(p) \right | }{\left | GT(p) \right |}	
%%recall@N(p) = \frac{\sum_{r=1}^{N}\left |  GT(p) \bigcap REC_{r}(p) \right |}{\left | GT(p) \right |}	
%\end{equation}
%%\vspace{-.1cm}
%



\subsection{Research Questions} \label{sec:ResearchQuestions}
By performing the evaluation, we aim at addressing the following research questions:
\begin{itemize}
	\item[--] \rqfirst To answer this question, we investigate different configurations to find the best one \ie we variate the number of input topics \emph{T}, the number of neighbours \emph{N} and the considered number of outcomes \emph{N}.
	
	%\item[--] \rqsecond Because of \CT and \MNB are completely different in term of input data, we are interested in comparing them by considering many factors that can impact on the performance.
	\item[--] \rqsecond From an empirical point of view, it is relevant to analyze the combination of the two approaches and measure its performances.
\end{itemize}


We study the experimental results in the next section by referring to these research questions.
