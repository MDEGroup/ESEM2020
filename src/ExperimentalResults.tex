This section discusses the findings of the qualitative assessment. To address the formulated research questions, we perform two different experiments. Section \ref{sec:EXP1} discusses the \CT results by variating different parameters. % We measure the predict performances of the \MNB in Section \ref{sec:EXP2}. 
The results obtained with the entangled approach (\ie the combination of \CT and \MNB approaches) are investigate in Section~\ref{sec:EXP3} . 


\subsection{\CT evaluation} \label{sec:EXP1}
 \rqfirst
 
To find the best configuration in terms of prediction performances, we experiment with different \CT configuration by variating the available parameters \ie number of neighbors \emph{k}, the recommended topic cut-off value \emph{N}, and the involved dataset.   %The former refers to the number of similar repositories used in the recommendation engine. 
%The latter value \emph{t} is used to select the input topics based on their frequencies: given an initial set of topics, we filter them with the cut-off value to reduce the noise in the original dataset. Then, the recommendation phase is enabled by varying the number of presented parameters. According to Section \ref{sec:methodology-metric}, N is the cut-off value and \emph{k} is the number of neighbours of the graph. We evaluate different configuration by setting N=1,5,10,15,20 and k=5,10,15,20,25. 
%Figure \ref{fig:configs} shows the results  in terms of precision and recall. 


As we are relying on a collaborative filtering technique, the number of output topics, the number of neighbours, and the data preprocessing play an important role in the assessment. Thus, we variate  the recommended list of topics \emph{N} for 5 and 10, and the number of neighbours \emph{k} \ie \emph{N} = \{5, 10, 15, 20, 25\}. 
%we use different topic frequency cut-off \emph{t} to remove very infrequent topics from the dataset. 
The bar charts in Fig.~\ref{fig:success-rateN5} and \ref{fig:success-rateN10} show the average success rates of all ten folds of \CT.%divided by the different topic frequency cut-off \emph{t}
Both figures depict the results of \CT applied on the different datasets defined in Section~\ref{sec:Dataset} \ie $Dt_{1}$, $Dt_{5}$, $Dt_{10}$, $Dt_{15}$, and $Dt_{20}$.
In particular, Fig.~\ref{fig:success-rateN5} and Fig.~\ref{fig:success-rateN10} shows the success rate considering the first 5 and 10 recommended topics respectively. The horizontal axes shows the success rate outcomes for different size of neighbours \emph{N}.
Overall, it is evident that infrequent topics negatively affect both success rate values. At the first glance we can see that the success rate of \CT with all topics is much lower than others \emph{t} cut-off.
\begin{figure*}[t]
\centering
	\begin{tabular}{c c}	
	\subfigure[Success rate@5]{\label{fig:success-rateN5}
	\includegraphics[width=0.45\linewidth]{figs/successRateN@5.pdf}} &
	\subfigure[Success rate@10]{\label{fig:success-rateN10}
	\includegraphics[width=0.45\linewidth]{figs/successRateN@10.pdf}}
	
	\end{tabular} 
	\caption{Success rate with 5 and 10 input topics.}
	\label{fig:success5_10}
\end{figure*}
\begin{figure}[t!]
	\centering
	\includegraphics[width=.95\linewidth]{figs/PrecisionRecallCurve.png}
	\caption{Evaluation of the different configuration.}
	\label{fig:configs}
\end{figure}
The success rate assessment exhibits an average improvement of 10\% in all of the possible configurations obtained by variating \emph{N} and \emph{k} values. In particular, the success rate archives better results by setting higher values of \emph{k}. Nevertheless, increasing the number of neighbours gives remarkable benefits only until a certain threshold. Given \emph{k} = 5, the success rate@5 passes from 63\% to 69\% if we consider k=10. This positive delta decreases by augmenting the number of neighbours until it reaches a stable success rate. Thus, we can consider \emph{k} = 25 as the maximum value capable of improving prediction performances. This trend is further confirmed by introducing more topics in the initial set. We also demonstrate that the topic filtering preprocessing fosters this enhancement and noise removal is a critical step of the entire process.

This is also confirmed by the precision and recall curves depicted in Fig.~\ref{fig:configs}. 
%From the accuracy scores computed using Eq. (5) and Eq. (6), the Precision-Recall curves (PRCs) for all 10 rounds of validation and different values of k were sketched. 
The line graph depicts the precision and recall curves on average for all 10 rounds by considering \emph{N} value ranges from 1 to 20 and \emph{t}. So, each dot in a curve corresponds to a specific value of \emph{N}. 
These outcomes have been obtained by keeping 25 as the number of neighbours \emph{k} because we have already discussed that higher values of neighbours reach better prediction performances. Overall, the precision and recall values rise when the \emph{t} cut-off grows. Given that better prediction performance appears near to the upper right corner~\cite{DiNoia:2012:LOD:2362499.2362501}, the figure shows that a higher value of \emph{t} reaches better accuracy for all values of \emph{N}.

As defined in Section~\ref{sec:metrics}, the coverage metric is the percent of recommended topic in the training data that the model is able to recommend on a test set. Table~\ref{tab:coverage} reports the average coverage value for all ten rounds.
For each dataset (\ie $Dt_{1}$, $Dt_{5}$, $Dt_{10}$, $Dt_{15}$, $Dt_{20}$), 
the table consists of two coverage values: the former (\ie \emph{Dataset coverage}) is computed by considering the coverage with respect to the number of topics after the filtering step, whereas the latter considers all mined topics (\ie 15,743 topics). The value of \emph{Dataset coverage} grows by increasing the topic frequency cut-off value \emph{t}, whereas the \emph{Global coverage} decreases because there are no training data to recommend infrequent topics. In particular, the average \emph{Global coverage} values decrease from 9.306\% (Dt$_1$) to 1.805\% (Dt$_{20}$).
Having a higher value of \emph{t}, it strongly impacts on the global catalog coverage value, because too many training data are discarded due to the topic frequency cut-off \emph{t}. Differently from the discussed metric outcomes, this exeriment shows how an higher values of topic frequency cut-off negatively impacts on the global catalog coverage value.
The values of \emph{Dataset coverage} does not provide the information about the original topics directly mined from \GH.


\JDR{Change this part} In the methodology described in Section~\ref{sec:methodology-metric}, for each repository \emph{r}, the evaluation outcomes consider the half part of real topics as input and remaining ones as ground truth data \emph{GT(r)}. Because of we are also interested to understand how the number of input topics impacts on prediction performance, Fig.~\ref{fig:pr-input-topics} shows the average success rate of all ten folds by choosing different number of input topics. Varying |\emph{t$_{in}$}| means changing the length of input topics that enable the \CT collaborative filtering recommender. In this picture we report the average success of all folds values for the best configuration settings (\ie \emph{k} = 25 and \emph{t} = 20) . The success rate values exhibits an improvements when the size of input topic rises. This  behaviour demonstrate that \CT computes better similar repositories as neighbours when it has a higher number of topic as input. This is due to the similarity function that has been involved in the computation of first  \emph{k} neighbours. Because the average number of topics for each considered repository is 9.896 we can consider |\emph{t$_{in}$}| = 5 as the maximum value capable of improving prediction performances.
\begin{figure}[t!]
	\centering
	\includegraphics[width=\linewidth]{figs/sr_change_input_topics.pdf}
	\caption{Evaluation of the different input topics.}
	\label{fig:pr-input-topics}
\end{figure} 

\input{src/coverageTable}

\begin{tcolorbox}[boxrule=0.86pt,left=0.3em, right=0.3em,top=0.1em, bottom=0.05em]
The quality evaluation demonstrates that \CT achieves better performance in terms of accuracy and success rate by increasing the number of considered neighbours \emph{k} and filtered data \emph{t}. %The number of neighbors and the topic filters contribute to this improvement. 
However, the conducted experiment shows that an higher values of topic frequency cut-off \emph{t} negatively impact on the global catalog coverage. 
For this reason, the \emph{t} values should be careful selected during the filtering phase to obtain balanced results in term of accuracy, success rate, and global catalog coverage.
However, the precision and recall values are still low, suggesting that bias lives in the users topic.
\end{tcolorbox}


%\input{src/oldRQ2}

\subsection{Entangled evaluation} \label{sec:EXP3}
\rqsecond

Due to the internal construction of the \MNB, the direct comparison of the two approaches can bring biased results. Thus, we combined the two approaches to investigate potential improvements. We create this \emph{entagled} configuration by feeding \CT with the results of the \MNB. This simulates the exact use case of the collaborative filtering approach, in which the developer is represented by the \MNB. Table \ref{tab:combined} summarizes the results of this experiment by comparing the \MNB and the entangled approach. From the previous assessment, we figured out that \CT reaches best results considering the Dataset $Dt_{20}$. Thus, we choose this one to conduct this second evaluation. \CDS{Check if it is enough, maybe we can extend the evaluation to another additional dataset}
For experiment purposes, we variate the number of recommendation items as well as the number of input topics \ie \emph{Out} and \emph{Tin} values respectively. From the previous assessment, we figured out that the number of inputs leading the best results is Tin=5. Thus, we compare the outcomes considering the minimum number of input topics provided by the \MNB, \ie Tin=2. The results demonstrate that the \MNB gains notable improvement by means of the entangled configuration in terms of the mentioned metrics \ie accuracy, success rate, and catalog coverage. We witness that \CT outperforms the \MNB by augmenting the number of recommended items. 

In particular, after Out=8 the accuracy and success rate overcomes the \MNB results considering the \CT's best configuration even though the overall accuracy trend is decreasing. This happens because enlarging the set of recommended items impacts negatively on the precision values. Reversely, the success rate rises up to 0.855 with the best configuration of the entangled approach. As witnessed for the accuracy value, the \MNB records better results until a certain threshold of output items. This degradation in performance is due to the internal probabilistic model used by the approach. 

\begin{table*}[]
	\small
	\begin{tabular}{|l | lll| lll |lll |lll|}
		\hline
		& \multicolumn{3}{l|}{\textbf{Recall}} & \multicolumn{3}{l|}{\textbf{Precision}} & \multicolumn{3}{l|}{\textbf{Success rate}} & \multicolumn{3}{l|}{ \textbf{Catalog coverage}} \\ \hline
		$N$  & \textbf{MNB}     & \textbf{Tin=5}   & \textbf{Tin=2}  & \textbf{MNB}      & \textbf{Tin=5 }   & \textbf{Tin=2}   & \textbf{MNB}       & \textbf{Tin=5 }   & \textbf{Tin=2}    & \textbf{MNB}        & \textbf{Tin=5 }     & \textbf{Tin=2}      \\ \hline
		%1  & 0,018   & 0,018   & 0,018  & 0,138    & 0,138    & 0,138   & 0,136     & 0,136    & 0,136    & 10.769     & 4.835      & 4.835      \\ \hline
		2  & 0.035   & 0.031   & 0.031  & 0.206    & 0.118    & 0.118   & 0.363     & 0.217    & 0.217    & 9.068     & 8.593      & 8.593      \\ \hline
		%3  & 0,047   & 0,047   & 0,060  & 0,118    & 0,118    & 0,151   & 0,301     & 0,301    & 0,367    & 19.780     & 12.483     & 12.571     \\ \hline
		4  & 0.075   & 0.063   & 0.088  & 0.221   & 0.119    & 0.166   &  0.600     & 0.389    & 0.466    & 19.405     & 15.340     & 15.912     \\ \hline
		%5  & 0,081   & 0,081   & 0,104  & 0,124    & 0,124    & 0,157   & 0,476     & 0,476    & 0,508    & 25.494     & 18.307     & 19.054     \\ \hline
		6  & 0.094   & 0.121   & 0.119  & 0.187    & 0.153    & 0.149   & 0.635    & 0.601    & 0.549    &  24.682     & 22.131     & 21.780     \\ \hline
		%7  & 0,112   & 0,149   & 0,131  & 0,121    & 0,161    & 0,140   & 0,605     & 0,668    & 0,574    & 28.791     & 25.912     & 24.681     \\ \hline
		\rowcolor{Gray}
		8  & 0.106   & 0.171   & 0.142  & 0.159  & 0.162    & 0.133   &  0.680     & 0.704    & 0.599    & 27.967     & 29.296     & 27.428     \\ \hline
		%9  & 0,135   & 0,189   & 0,153  & 0,115    & 0,160    & 0,128   & 0,678     & 0,734    & 0,623    & 29.230     & 32.417     & 30.307     \\ \hline
		10 & 0.116   & 0.204   & 0.163  & 0.140    & 0.156    & 0.123   & 0.701     & 0.754    & 0.644    & 30.719     & 35.296     & 32.967     \\ \hline
		12 & 0.124   & 0.230   & 0.181  & 0.124    & 0.146    &  0.114   & 0.719     & 0.788    & 0.681    & 32.786     & 40.659     & 38.373     \\ \hline
		14 & 0.130   & 0.254   & 0.201  & 0.111    & 0.138    & 0,109   &  0.733     & 0.808    & 0.706    & 34.308     & 45.912     & 43.098     \\ \hline
		16 & 0.135   & 0.274   & 0.215  &  0.101    & 0.131    & 0.102   & 0.745    & 0.829    & 0.722    & 35.742     & 50.505     & 47.582     \\ \hline
		18 & 0.143   & 0.290   & 0.227  & 0.095   & 0.123    & 0.096   & 0.759     & 0.840    & 0.736    & 37.644     & 54.615     & 51.318     \\ \hline
		20 & 0.150   & 0.306   & 0.241  & 0.090    & 0.117    & 0.092   & 0.772     & 0.855    & 0.756    & 39.636    & 58.725     & 54.923    \\ \hline
	\end{tabular}
\vspace{.2cm}
\caption{Results for the entangled approach for the Dataset $Dt_{20}$.}
\label{tab:combined}
\end{table*}













%\begin{table}[h]
%\centering
%
%
%\resizebox{8.5cm}{!} {
%\begin{tabular}{|l|l|l|l|l|l|l|}
%\hline
%  & \multicolumn{3}{c|}{\textbf{\CT}}          & \multicolumn{3}{c|}{\textbf{Entangled approach}}        \\ \hline
%\textbf{No. of input} & \textbf{Success rate} &\textbf{ Precision} & \textbf{Recall} & \textbf{Success rate} &\textbf{ Precision} & \textbf{Recall} \\ \hline
%1  &       0.409       &    0.409       &  0.105       &     0.138         &      0.221     &   0.029      \\ \hline
% 2 &     0.554         &    0.350       &     0.179   &       0.220       &       0.198    &   0.053     \\ \hline
%3 &    0.632          &      0.301	     &   0.230     &     0.304         &    0.192       &   0.077     \\ \hline
%4 &    0.682          &  0.267         &   0.271      &         0.393    &     0.186      &   0.099     \\ \hline
%5 &      0.728        &    0.246       &   0.310     &      0.479        &   0.183        &    0.122    \\ \hline
%\rowcolor{Gray}
%6 &     0.754         &      0.224     &   0.339     &        0.983      &     0.278      &     0.225   \\ \hline
%7 &      0.778        &    0.207       &   0.363     &    0.999          &   0.340         &   0.322     \\ \hline
%8 &       0.803       &     0.192      &     0.384   &        1      &   0.371        &   0.40     \\ \hline
%10 &      0.828        &     0.169      &     0.422   &       1       &     0.382      &    0.511    \\ \hline
%15 &     0.872         &    0.132       &    0.493    &      1        &   0.322       &       0.636 \\ \hline
%20 &     0.892         &    0.117       &    0.537    &      1        &   0.266        &       0.696 \\ \hline
%\rowcolor{Gray}
%\textbf{Average values} &    \textbf{ 0.785}        &   \textbf{ 0.194}       &   \textbf{ 0.397}   &     \textbf{ 0.826 }       &  \textbf{ 0.296}        &       \textbf{0.433}  \\ \hline
%\end{tabular}
%}
%\caption{Results for the entangled approach.}
%\label{tab:combined}
%\end{table} 



Although the examined metrics are useful to analyze the overall performances, the catalog coverage can evaluate properly the capability to recommend a \emph{list} of items instead of a single one. Looking at the results, we can observe a substantial increase after 8 output items. As expected, the coverage dramatically increases with a larger number of outcomes for both of the considered approaches. Nevertheless, the positive gap of the entangled configuration is greater than the \MNB value. Considering the Out=20, the maximum value reached by the \MNB is 39.636 while the best configuration in the entangled experiment reaches a coverage of 58.725.

These findings can be explained by considering the nature of the considered topics. As said before, the \MNB can predict only featured topics as training the entire set of \GH topics is not possible due to the computation issues. Reversely, \CT covers a larger set of topics by enabling the described collaborative filtering technique. In this way, the \emph{entangled} is capable of suggesting both featured and not featured topics to the final user and enlarging the possible set of outcomes.



\begin{tcolorbox}[boxrule=0.86pt,left=0.3em, right=0.3em,top=0.1em, bottom=0.05em]
The entangled approach success in improving the prediction performances. By variating both the input and output number of topics, the accuracy and success rate experienced an enhancement even though the former reached low values. The \MNB lacks in catalog coverage, as clearly demonstrated by the higher value of the entangled experiment.
\end{tcolorbox}










