
This section discusses the threats that may affect the results of the evaluation. We also list the countermeasures taken to minimize these issues.

The \emph{internal validity} could be compromised by the dataset features, \ie the number of projects for each topic, the number of results. We tackle this issue by varying the aforementioned parameters to build datasets with different characteristics. In this way, several settings have been used to evaluate \TF's overall performances.

\emph{External validity} concerns the rationale behind the selection of the \GH repositories used in the assessment. As stated in the Section~\ref{sec:RelatedWorks}, we randomly downloaded repositories by imposing a quality filter on the number stars. Nevertheless, some repositories could be tagged with topics that can affect the quality of the graph computed in the data extraction phase. To be concrete, a user can label a repository using terms that are not descriptive enough, \ie using infrequent or duplicated terms in the topic list. To deal with this issue, we applied the topic filter as described in Section \ref{sec:Dataset} to reduce any possible noise during the graph construction phase.

Threats to \emph{construction validity} are related to the choice of \MNB as the baseline in the conducted experiments. First of all, the availability of the replication package allows us to perform a comprehensive evaluation. %rather than other approaches. 
As we claimed before, the two approaches are strongly different from the construction point of view including the recommendation engine and data extraction components. To make the comparison as fair as possible, we ran \MNB on the same datasets by adapting the overall structure for the ten-fold cross-validation evaluations.